
import tensorflow as tf
from mate import PeriodicSelfAttention
from batch_geneartor import BatchGenerator
import numpy as np
import os


class MetricTracker:
    def __init__(self, class_num, file_stream=None):
        self.file_stream = file_stream
        self.class_num = class_num
        self.acc_count = 0
        self.acc_true = 0
        self.loss_sum = 0.0
        self.mse = 0.0
        self.acc_count_prec_per_class = [0 + 1e-10] * class_num
        self.acc_per_class = [0] * class_num
        self.acc_count_recl_per_class = [0 + 1e-10] * class_num
        self.confusion_matrix = np.zeros([class_num, class_num], dtype=int)
        if file_stream is not None and os.path.exists(file_stream):
            os.remove(file_stream)

    def reset_zero(self):
        self.acc_count = 0
        self.acc_true = 0
        self.loss_sum = 0.0
        self.mse = 0
        self.acc_count_prec_per_class = [0 + 1e-10] * self.class_num
        self.acc_per_class = [0] * self.class_num
        self.acc_count_recl_per_class = [0 + 1e-10] * self.class_num
        self.confusion_matrix = np.zeros([self.class_num, self.class_num], dtype=int)

    def update(self, logits, label, loss):
        y_pred = np.argmax(logits, axis=1)
        y_real = label
        n_sample = len(y_real)
        self.loss_sum += loss*n_sample
        self.acc_count += n_sample
        for i in range(n_sample):
            self.acc_count_prec_per_class[y_pred[i]] += 1
            self.acc_count_recl_per_class[y_real[i]] += 1
            self.confusion_matrix[y_real[i], y_pred[i]] += 1
            if y_pred[i] == y_real[i]:
                self.acc_true += 1
                self.acc_per_class[y_pred[i]] += 1

    def print_info(self, name, epoch_i, batch_i, max_epoch, max_batch, is_acc=True, is_rmse=False,
                   is_each_class=False, is_confusion_mat=False, is_weighted_f_score=False):
        print_str = 'Epoch {:>3}/{} Batch {:>4}/{} - '.format(epoch_i, max_epoch, batch_i, max_batch)
        print_str += 'Loss: {:>6.3f} - '.format(self.loss_sum / self.acc_count)
        if is_acc:
            print_str += '{} Acc: {:>6.3f} - '.format(name, self.acc_true / self.acc_count)
        if is_rmse:
            print_str += '{} RMSE: {:>6.3f} - '.format(name, np.sqrt(self.mse / self.acc_count))
        if is_each_class:  # output results for each class
            for i in range(1, self.class_num):
                print_str += '\n{} Precision of Class {:>3d}: {:d}/{:d}={:>6.3f}' \
                    .format(name, i, self.acc_per_class[i], self.acc_count_prec_per_class,
                            self.acc_per_class[i] / self.acc_count_prec_per_class[i])
            for i in range(1, self.class_num):
                print_str += '\n{} Recall of Class {:>3d}: {:d}/{:d}={:>6.3f}' \
                    .format(name, i, self.acc_per_class[i], self.acc_count_recl_per_class,
                            self.acc_per_class[i] / self.acc_count_recl_per_class[i])
        if is_confusion_mat:
            print_str += '\n{} Confusion Matrix:'.format(name)
            for i in range(1, self.class_num):
                print_str += '\n' + '\t'.join(str(each) for each in self.confusion_matrix[i][1:])
        if is_weighted_f_score:
            f_score = np.zeros(self.class_num)
            for i in range(1, self.class_num):
                f_score[i] = 2 * self.acc_per_class[i] / \
                             (self.acc_count_prec_per_class[i] + self.acc_count_recl_per_class[i])
            weighted_f_score = np.average(f_score[1:], weights=self.acc_count_recl_per_class[1:])
            print_str += '{} Weighted-F: {:>6.3f} - '.format(name, weighted_f_score)

        print(print_str)
        if self.file_stream is not None:
            with open(self.file_stream, 'a') as f:
                print(print_str, file=f)


class ModelTrainer:
    def __init__(self, hp):
        self.lrate = hp.learning_rate
        self.max_epoch = hp.epochs
        self.batch_size = hp.batch_size
        self.train_size = hp.train_size
        self.test_size = hp.test_size
        self.max_batchsize = self.train_size // self.batch_size
        self.max_batchsize_test = self.test_size // self.batch_size

        self.model = PeriodicSelfAttention(hp)
        self.batch_generator = BatchGenerator(hp, make_portraits_zero=True if hp.model_tag == 'empty' else False)
        self.train_metric_tracker = MetricTracker(class_num=self.model.class_num)
        self.test_metric_tracker = MetricTracker(class_num=self.model.class_num)

        self.hp = hp

    # todo: can run without this function!!!
    def __input_placeholder(self):
        with tf.name_scope('input'):
            # input
            x_input = tf.placeholder(dtype=tf.int32, shape=[None, self.model.maxlen], name='x_input')
            time_input = tf.placeholder(dtype=tf.float32, shape=[None, self.model.maxlen], name='time_input')
            portrait_input = tf.placeholder(dtype=tf.float32, shape=[None, self.model.portrait_vec_len],
                                            name='portrait_input')
            daily_periodic_input = tf.placeholder(dtype=tf.int32,
                                                  shape=[None, self.model.daily_periodic_len, self.model.maxlen],
                                                  name='daily_periodic_input')
            weekly_periodic_input = tf.placeholder(dtype=tf.int32,
                                                   shape=[None, self.model.weekly_periodic_len, self.model.maxlen],
                                                   name='weekly_periodic_input')
            role_id_seq = tf.placeholder(tf.int32, [None], name='role_id_seq')
            is_training = tf.placeholder(tf.bool, name='is_training')

            # label
            y_output = tf.placeholder(tf.int32, [None], name='y_output')
            time_label = tf.placeholder(tf.float32, [None], name='time_label')

            if self.model.weekly_periodic_len == 0:
                weekly_periodic_input = daily_periodic_input

        return x_input, time_input, portrait_input, daily_periodic_input, weekly_periodic_input, role_id_seq,\
            is_training, y_output, time_label

    def model_train(self):
        with tf.name_scope('optimization'):
            with self.model.graph.as_default():
                train_dataset = tf.data.Dataset.from_generator(lambda: self.batch_generator.get_batch(
                                                                       datatype='training'),
                                                               (tf.int32, tf.int32, tf.float32, tf.float32, tf.float32,
                                                                tf.int32),
                                                               (tf.TensorShape([None, self.model.maxlen]),
                                                                tf.TensorShape([None]),
                                                                tf.TensorShape([None, self.model.maxlen]),
                                                                tf.TensorShape([None]),
                                                                tf.TensorShape([None, self.model.portrait_vec_len]),
                                                                tf.TensorShape([None]),
                                                                ))
                test_dataset = tf.data.Dataset.from_generator(lambda: self.batch_generator.get_batch(
                                                                      datatype='testing'),
                                                              (tf.int32, tf.int32, tf.float32, tf.float32, tf.float32,
                                                               tf.int32,),
                                                              (tf.TensorShape([None, self.model.maxlen]),
                                                               tf.TensorShape([None]),
                                                               tf.TensorShape([None, self.model.maxlen]),
                                                               tf.TensorShape([None]),
                                                               tf.TensorShape([None, self.model.portrait_vec_len]),
                                                               tf.TensorShape([None]),
                                                               ))

                train_dataset = train_dataset.prefetch(1)
                test_dataset = test_dataset.prefetch(1)

                iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)

                x_input, y_output, time_input, time_label, portrait_input, role_id_seq = iterator.get_next()

                training_init_op = iterator.make_initializer(train_dataset)
                testing_init_op = iterator.make_initializer(test_dataset)

                is_training = tf.placeholder(tf.bool, name='is_training')
                logits, cost = self.model.get_loss(x_input, time_input, portrait_input,
                                                   role_id_seq, y_output, is_training=is_training)

                tf.summary.scalar('loss', cost)

                # Optimizer
                global_steps = tf.Variable(0, name='global_step', trainable=False)
                train_op = tf.train.AdamOptimizer(self.lrate, beta1=0.9,
                                                  beta2=0.999, epsilon=1e-8).minimize(cost, global_step=global_steps)
            with tf.Session(graph=self.model.graph) as session:
                config = tf.ConfigProto(inter_op_parallelism_threads=self.hp.inter_op_parallelism_threads,
                                        intra_op_parallelism_threads=self.hp.intra_op_parallelism_threads)
                sess = tf.Session(config=config)
                sess.run(tf.global_variables_initializer())
                sess.run(tf.local_variables_initializer())

                epoch_i = 1
                while epoch_i <= self.max_epoch:
                    self.train_metric_tracker.reset_zero()
                    self.test_metric_tracker.reset_zero()

                    with tf.name_scope('loss'):
                        # training
                        sess.run(training_init_op)
                        for batch_i in range(self.max_batchsize):
                            _, logits_, loss_, y_batch = \
                                sess.run([train_op, logits, cost, y_output],
                                         {is_training: True})
                            self.train_metric_tracker.update(logits_, y_batch, loss_)

                        # testing
                        sess.run(testing_init_op)
                        for test_batch_i in range(self.max_batchsize_test):
                            logits_, loss_, y_test_batch = \
                                sess.run([logits, cost, y_output],
                                         {is_training: False})
                            self.test_metric_tracker.update(logits_, y_test_batch, loss_)

                        # print performance info per epoch
                        self.train_metric_tracker.print_info('Train', epoch_i, (batch_i % self.max_batchsize) + 1,
                                                             self.max_epoch,
                                                             self.max_batchsize,
                                                             is_weighted_f_score=True)
                        self.test_metric_tracker.print_info('Test', epoch_i,
                                                            (test_batch_i % self.max_batchsize_test) + 1,
                                                            self.max_epoch,
                                                            self.max_batchsize_test,
                                                            is_weighted_f_score=True)

                    epoch_i += 1
